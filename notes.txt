






In this post, we will focus on how to set up an ECS cluster of EC2 instances using Terraform.

We will cover

How to deploy a service on ECS clusters
ECS deployment with Terraform – Overview
How to set up ECS with Terraform – Example
1. Setting up the VPC
2. Configuring the EC2 instances
3. Configuring the ECS cluster
4. Testing the ECS deployment
How to deploy a service on ECS clusters
There are mainly two ways of deploying a service on ECS clusters – Fargate and EC2 instances. This depends on the underlying infrastructure used to run the container workloads of any ECS service.

AWS Fargate is a more cloud-native approach where the compute instances are automatically managed by AWS.

Running the ECS service on EC2 instances provides more control over the infrastructure. This also requires taking additional steps to set up the EC2 instances and auto-scaling group, networking, etc.

An ECS service generally consists of the components shown in the diagram below.

ecs task definition terraform
When an ECS Cluster is created, before any service is deployed, we have to provide the cluster with capacity providers.

In this blog post, the capacity providers are the EC2 instances where the scaling is managed by ASG.  The service can then be deployed once the capacity providers or the capacity providing EC2 instances are registered with the ECS cluster.

An ECS service consists of multiple tasks. Each task is created based on the task definition provided by the service.

A task definition is a template that describes the source of the application image, resources required in terms of CPU and memory units, container and host port mapping, and other critical information.

Apart from the task definition, ECS service also provides the number of container instances to be created. This automatically creates the tasks, which are then assigned a target EC2 instance infrastructure where the containers are run. The running containers serve the incoming requests from the application load balancer (ALB) which is deployed in front of the EC2 instances in a VPC.

Why use ECS over EC2?
ECS and EC2 are services provided by AWS for running applications in the cloud. Although at a high level, they seem to host workloads, they serve different purposes and are suited for different types of workloads.

Some of the reasons for choosing ECS over EC2 are described below:

Containerization: ECS is designed for running containers either on Fargate or EC2 instances, which provide a scalable way to package and deploy applications. Containers offer better resource utilization, faster startup times, and improved isolation compared to traditional virtual machines used in EC2. If you’re using containerized applications, ECS provides a managed environment optimized for running and orchestrating containers.
Scalability and Elasticity: ECS allows you to easily scale your applications based on demand. It integrates with AWS Auto Scaling, which can automatically adjust the number of containers based on predefined scaling policies and placement constraints.
Orchestration: ECS provides built-in orchestration capabilities through integration with AWS Fargate or EC2 launch types. With Fargate, we don’t have to provision or manage any EC2 instances, as AWS takes care of the infrastructure for you. This is also a great option and allows us to focus solely on our applications. EC2 launch type offers more control and flexibility if we prefer managing the underlying EC2 instances ourself.
Monitoring: ECS provides a centralized management console and CLI tools for deploying and managing containers. It also integrates with AWS CloudWatch, allowing us to collect and analyze metrics, monitor logs, and set up alarms for our containerized applications. EC2 instances require separate management and monitoring configurations.
Cost Optimization: ECS can help optimize costs by allowing us to run containers on-demand without the need for permanent EC2 instances. With AWS Fargate, we pay only for the resources allocated to our containers while they are running, which is more cost-effective compared to maintaining a fleet of EC2 instances. Read more about AWS cost optimization.
It’s important to note that the choice between ECS and EC2 depends on our specific requirements, workload characteristics, and familiarity with containerization. While ECS offers benefits for container-based applications, EC2 still provides more flexibility for running traditional workloads that don’t require containerization.

ECS deployment with Terraform - Overview
The diagram below shows the outcome of ECS deployment using Terraform.

It shows how an ECS cluster is set up on EC2 instances spread across multiple availability zones within a VPC. It also includes several details like Application Load Balancers (ALB), auto-scaling group (ASG), ECS Capacity provider, ECS service, etc.

Note that there are other aspects not represented in the diagram, like ECR, Route tables, task definition, etc., which we will also cover.

terraform aws ecs
We will break this infrastructure down into three parts. The list below provides an overview at a high level of all the steps we will be taking to achieve our goal of hosting ECS clusters on EC2 instances. As we proceed, we will also write the corresponding Terraform configuration.

VPC setup – In this part, we will explain and implement the basic VPC and networking setup required. We will implement subnets, security groups, and route tables, to access the hosted service from the internet as well as to SSH into our EC2 instances. 
EC2 setup – In this part, we will explain and implement auto-scaling groups, application load balancer, and EC2 instances, across the two availability zones. We will also cover the setup in details required to host ECS container instances on our EC2 machines.
ECS setup – Finally, we do a step-by-step creation of the ECS cluster by creating ECS clusters, services, tasks, and capacity providers. We will also take a look at the application image which will be used to run on this ECS cluster. Additionally, we will also show how to create task definitions along with various Terraform resource attributes which enable the end-to-end deployment of the service.
Note: The final Terraform configuration is saved in this Github repository.

Before moving ahead, it is highly recommended to follow these steps hands-on.

How to setup ECS with Terraform - Example
Before we start, you should have Terraform installed locally. We will not go through the details of setting up the providers, AWS credentials, and AWS CLI.

Let’s go through the steps to deploy ECS using Terraform.

1. Setting up the VPC
To begin with, let us first establish our isolated network by defining the VPC and related components. This is important as we need to create multiple container instances of our application so that the load balancer is able to distribute the requests evenly.

The diagram below displays the target VPC design we would achieve using Terraform.

aws ecs service terraform
Create a file named vpc.tf in the project repository and add the code below.

You can optionally include all the Terraform code in the same file (e.g., in main.tf). However, I prefer to keep separate files to manage the IaC easily. 

Step 1: Define our VPC
The code below creates a VPC with a given CIDR range. You are free to choose a CIDR range.

As an example, we use 10.0.0.0/16 in this case and name our VPC as “main”.

resource "aws_vpc" "main" {
 cidr_block           = var.vpc_cidr
 enable_dns_hostnames = true
 tags = {
   name = "main"
 }
}
Step 2: Add 2 subnets
Create two subnets in different availability zones to place our EC2 instances. We have used “cidrsubnet” Terraform function to dynamically calculate the CIDR range based on the VPC’s CIDR.

Note that we are using different availability zones in the Frankfurt region for both subnets.

resource "aws_subnet" "subnet" {
 vpc_id                  = aws_vpc.main.id
 cidr_block              = cidrsubnet(aws_vpc.main.cidr_block, 8, 1)
 map_public_ip_on_launch = true
 availability_zone       = "eu-central-1a"
}

resource "aws_subnet" "subnet2" {
 vpc_id                  = aws_vpc.main.id
 cidr_block              = cidrsubnet(aws_vpc.main.cidr_block, 8, 2)
 map_public_ip_on_launch = true
 availability_zone       = "eu-central-1b"
}
Step 3: Create internet gateway (IGW)
resource "aws_internet_gateway" "internet_gateway" {
 vpc_id = aws_vpc.main.id
 tags = {
   Name = "internet_gateway"
 }
}
Step 4: Create a Route table and associate the same with subnets
This route table enables internet communication of both the subnets, making them public.

resource "aws_route_table" "route_table" {
 vpc_id = aws_vpc.main.id
 route {
   cidr_block = "0.0.0.0/0"
   gateway_id = aws_internet_gateway.internet_gateway.id
 }
}

resource "aws_route_table_association" "subnet_route" {
 subnet_id      = aws_subnet.subnet.id
 route_table_id = aws_route_table.route_table.id
}

resource "aws_route_table_association" "subnet2_route" {
 subnet_id      = aws_subnet.subnet2.id
 route_table_id = aws_route_table.route_table.id
}
Step 5: Create a security group along with ingress and egress rules
Both ingress and egress rules of the security group allow inbound and outbound access for any protocol, via any port. This is not the best practice and should only be done for working through this example. Tighter rules should be implemented when in production.

resource "aws_security_group" "security_group" {
 name   = "ecs-security-group"
 vpc_id = aws_vpc.main.id

 ingress {
   from_port   = 0
   to_port     = 0
   protocol    = -1
   self        = "false"
   cidr_blocks = ["0.0.0.0/0"]
   description = "any"
 }

 egress {
   from_port   = 0
   to_port     = 0
   protocol    = "-1"
   cidr_blocks = ["0.0.0.0/0"]
 }
}
At this point, this is all we need as far as the VPC and networking is concerned. You can go ahead and provision these resources now or proceed with the next section.

The VPC setup is quite straightforward. We will not go deep into exploring the VPC creation using Terraform, as that is not the topic of this blog post. For more information on that, check out How to Build AWS VPC using Terraform.

This gives us the networking basics to proceed to the next step.

2. Configuring the EC2 instances
In this section, we are focusing on provisioning the auto-scaling group, defining the EC2 instance template used to host the ECS containers, and provisioning the application load balancer in the VPC created in the previous section.

Below you can see the updated diagram.

 

terraform ecs cluster example
Create another file named ec2.tf, and follow the steps below. 

Step 1: Create an EC2 launch template
A launch template, as the name suggests, defines the template used by the auto-scaling group to provision and maintain a desired/required number of EC2 instances in a cluster.

Launch templates define various characteristics of the EC2 instance:

Image: We use Amazon Linux image with CPU architecture as AMD.
Type: Size of the instance. We use “t3.micro”.
The size of the instance is decided by the system resources consumed by the container. In our case, we are hosting a “Docker Getting Started” image which does not consume a lot of resources. More about the image will be covered later in the post.
Key name: Specify the name of the key to be able to SSH into these instances from our local machines. You can either create a key using a separate Terraform resource block. In this case, I am just using a key that I have already generated.
Security group: The security group to be associated with the EC2 instance. Associate the same security group we created in the previous section.
IAM instance profile: This is very important. Without this, the EC2 instances will not be able to access the ECS service in AWS. “ecsInstanceRole” is a predefined role available in all the AWS accounts. However, if you want to use a custom role, make sure it can access the ECS service.
User data: This is also very important. The “ecs.sh” file contains a command to create an environment variable in “/etc/ecs/ecs.config” file on each EC2 instance that will be created. Without setting this, the ECS service will not be able to deploy and run containers on our EC2 instance.
resource "aws_launch_template" "ecs_lt" {
 name_prefix   = "ecs-template"
 image_id      = "ami-062c116e449466e7f"
 instance_type = "t3.micro"

 key_name               = "ec2ecsglog"
 vpc_security_group_ids = [aws_security_group.security_group.id]
 iam_instance_profile {
   name = "ecsInstanceRole"
 }

 block_device_mappings {
   device_name = "/dev/xvda"
   ebs {
     volume_size = 30
     volume_type = "gp2"
   }
 }

 tag_specifications {
   resource_type = "instance"
   tags = {
     Name = "ecs-instance"
   }
 }

 user_data = filebase64("${path.module}/ecs.sh")
}
The contents of ecs.sh file are:

#!/bin/bash
echo ECS_CLUSTER=my-ecs-cluster >> /etc/ecs/ecs.config
Step 2: Create an auto-scaling group (ASG)
Create an ASG and associate it with the launch template created in the last step.

ASG automatically manages the horizontal scaling of EC2 instances as per what is required by the ECS service but within the limits defined in this resource block.

resource "aws_autoscaling_group" "ecs_asg" {
 vpc_zone_identifier = [aws_subnet.subnet.id, aws_subnet.subnet2.id]
 desired_capacity    = 2
 max_size            = 3
 min_size            = 1

 launch_template {
   id      = aws_launch_template.ecs_lt.id
   version = "$Latest"
 }

 tag {
   key                 = "AmazonECSManaged"
   value               = true
   propagate_at_launch = true
 }
}
Note that apart from the desired capacity, max, and min count, we have also specified the “vpc_zone_identifier” attribute. This limits the ASG to provision instances in the same availability zones where the subnets are created.

A region may have more than two availability zones. Since we are leveraging only two subnets, the vpc_zone_identifier should follow the appropriate AZs dynamically.

Step 3: Configure Application Load Balancer (ALB)
An ALB is required to test our implementation in the end. In a way, it is optional as far as the discussion of this blog post is concerned, but there is no fun in doing the hard work and not being able to witness it in the real world.

See How to Manage Application Load Balancer (ALB) with Terraform.

Create the ALB, its listener, and the target group as defined in the code samples below.

resource "aws_lb" "ecs_alb" {
 name               = "ecs-alb"
 internal           = false
 load_balancer_type = "application"
 security_groups    = [aws_security_group.security_group.id]
 subnets            = [aws_subnet.subnet.id, aws_subnet.subnet2.id]

 tags = {
   Name = "ecs-alb"
 }
}

resource "aws_lb_listener" "ecs_alb_listener" {
 load_balancer_arn = aws_lb.ecs_alb.arn
 port              = 80
 protocol          = "HTTP"

 default_action {
   type             = "forward"
   target_group_arn = aws_lb_target_group.ecs_tg.arn
 }
}

resource "aws_lb_target_group" "ecs_tg" {
 name        = "ecs-target-group"
 port        = 80
 protocol    = "HTTP"
 target_type = "ip"
 vpc_id      = aws_vpc.main.id

 health_check {
   path = "/"
 }
}
Note that we are still using the same VPC, subnets, and security group we created in the previous section. The rest of the ALB configuration is straightforward and basic.

Again, at this point, you can choose to create these resources using terraform plan and apply commands. Or, move forward for the final section, we would create the ECS cluster.

💡 You might also like:

5 Ways to Manage Terraform at Scale
How to Automate Terraform Deployments
Common Infrastructure Challenges and How to Solve Them
3. Configuring the ECS cluster
With the networking and EC2 infrastructure defined or provisioned, we finally have arrived at provisioning an ECS cluster and hosting a web application on the same.

In this section, we will achieve the target architecture represented in the target deployment section as well as below.

terraform aws ecs
Step 1: Create and push the application image
We would deploy the application “docker/getting-started”, which is usually shipped with every Docker desktop installation. Pull this image locally, and then push it to any accessible image repository of your choice.

To keep this simple, I have created a public Elastic Container Repository (ECR), tagged the image appropriately, and pushed the same to this public ECR.

Note that if you are using a Mac M1 processor (or any ARM-based processor) to build the image locally, then the CPU architecture differs from the Amazon Linux AMI (X86), which we have used in the EC2 setup section. This impacts the way images are built and are not compatible. As a workaround, do a manual build and push by logging in to an Amazon Linux AMI-based EC2 instance.

Step 2: Provision ECS cluster
Optionally, create a new configuration file and add the code below. In my example, I have created a main.tf file to add the ECS-related configuration. We begin by provisioning an ECS cluster using the “aws_ecs_cluster” resource.

resource "aws_ecs_cluster" "ecs_cluster" {
 name = "my-ecs-cluster"
}
This is a simple resource block with just a name attribute. This does not do much, but this is where provisioning an ECS cluster begins.

Step 3: Create capacity providers
Next, we create a couple of resources to provision capacity providers for the ECS cluster created in the previous step.

The “aws_ecs_capacity_provider” resource associates the auto-scaling group with the cluster’s capacity provider. Whereas “aws_ecs_cluster_capacity_providers” binds the ASG capacity provider with the ECS cluster created in Step 1.

resource "aws_ecs_capacity_provider" "ecs_capacity_provider" {
 name = "test1"

 auto_scaling_group_provider {
   auto_scaling_group_arn = aws_autoscaling_group.ecs_asg.arn

   managed_scaling {
     maximum_scaling_step_size = 1000
     minimum_scaling_step_size = 1
     status                    = "ENABLED"
     target_capacity           = 3
   }
 }
}

resource "aws_ecs_cluster_capacity_providers" "example" {
 cluster_name = aws_ecs_cluster.ecs_cluster.name

 capacity_providers = [aws_ecs_capacity_provider.ecs_capacity_provider.name]

 default_capacity_provider_strategy {
   base              = 1
   weight            = 100
   capacity_provider = aws_ecs_capacity_provider.ecs_capacity_provider.name
 }
}
Step 4: Create ECS task definition with Terraform
As described in the ECS overview section before, we now define the task definition/template of the container tasks to be run on the ECS cluster using the image we pushed in Step 1.

Some of the important points to note here are:

We have defined the network mode to be “awsvpc”. This tells the ECS cluster to use the VPC networking we have defined in the “VPC setup” section.
We have provided the task definition with ecsTaskExecutionRole.
Defined CPU resource requirement as 256.
The runtime platform is an important attribute. Since we are using Amazon Linux AMI for our EC2 instances, the operating_system_family is specified as “LINUX,” and the CPU architecture is set as “X86_64”. If this information is incorrect, the ECS tasks enter in the constant restart loop.
Container definitions: This is where we define the resource requirements of the container to be run in the task. We have provided below attributes:
Name of the container instance
Image URL of the application image
CPU capacity units
Memory capacity units
Container and host port mappings
resource "aws_ecs_task_definition" "ecs_task_definition" {
 family             = "my-ecs-task"
 network_mode       = "awsvpc"
 execution_role_arn = "arn:aws:iam::532199187081:role/ecsTaskExecutionRole"
 cpu                = 256
 runtime_platform {
   operating_system_family = "LINUX"
   cpu_architecture        = "X86_64"
 }
 container_definitions = jsonencode([
   {
     name      = "dockergs"
     image     = "public.ecr.aws/f9n5f1l7/dgs:latest"
     cpu       = 256
     memory    = 512
     essential = true
     portMappings = [
       {
         containerPort = 80
         hostPort      = 80
         protocol      = "tcp"
       }
     ]
   }
 ])
}
Step 5: Create the ECS service
This is the last step, where we provision the service to be run on the ECS cluster. This is where all the resources created culminate to successfully run the application service.

The attributes defined here are:

Name: name of the ECS service
Cluster: Reference to the ECS cluster record we created in Step 2.
Task definition: Reference to the task definition template created in Step 4.
Desired count: We have specified that we want to run two instances of this container image on our ECS cluster.
Network configuration: We have specified the subnets and security group we created in the VPC setup section.
Placement constraints: We have specified that the container instances should run on distinct instances instead of using the residual capacity in each instance. This is not a best practice, but just to prove the concept.
Capacity provider strategy: We have provided the reference to the capacity provider created in Step 3.
Load balancer: Reference to the load balancer we created in the EC2 setup section.
resource "aws_ecs_service" "ecs_service" {
 name            = "my-ecs-service"
 cluster         = aws_ecs_cluster.ecs_cluster.id
 task_definition = aws_ecs_task_definition.ecs_task_definition.arn
 desired_count   = 2

 network_configuration {
   subnets         = [aws_subnet.subnet.id, aws_subnet.subnet2.id]
   security_groups = [aws_security_group.security_group.id]
 }

 force_new_deployment = true
 placement_constraints {
   type = "distinctInstance"
 }

 triggers = {
   redeployment = timestamp()
 }

 capacity_provider_strategy {
   capacity_provider = aws_ecs_capacity_provider.ecs_capacity_provider.name
   weight            = 100
 }

 load_balancer {
   target_group_arn = aws_lb_target_group.ecs_tg.arn
   container_name   = "dockergs"
   container_port   = 80
 }

 depends_on = [aws_autoscaling_group.ecs_asg]
}
Run terraform plan and apply commands to provision all the infrastructure defined so far.



ssh -i ecs-instance-key.pem ec2-user@<instance-public-ip>

1. Remove the Terraform State Files
Delete the terraform.tfstate and terraform.tfstate.backup files:

sh
Copy code
rm terraform.tfstate terraform.tfstate.backup
2. Remove the .terraform Directory
Delete the .terraform directory and its contents:

sh
Copy code
rm -rf .terraform
3. Remove the Lock File
Delete the .terraform.lock.hcl file if it exists:

sh
Copy code
rm -f .terraform.lock.hcl
4. Initialize Terraform
Reinitialize your Terraform working directory:

sh
Copy code
terraform init
5. Review and Update Your Variables
Check and update your variables.tf file to ensure all necessary variables are defined and have appropriate default values. Uncomment any variables that you need for your configuration. This step involves editing your configuration files manually, so here’s an example of what you might need to do:

Example variables.tf File:

hcl
Copy code
# variables.tf

variable "vpc_cidr" {
  description = "The CIDR block for the VPC"
  type        = string
  default     = "10.0.0.0/16"
}

variable "availability_zones" {
  description = "List of availability zones for the VPC"
  type        = list(string)
  default     = ["a1", "b1"]  # Example values
}

# Uncomment or add any other necessary variables
Update Your Main Configuration:

Make sure your main.tf or other Terraform configuration files are updated as needed based on your reviewed variables and any additional configurations you require.

Summary of Commands:
Remove State Files:

sh
Copy code
rm terraform.tfstate terraform.tfstate.backup
Remove .terraform Directory:

sh
Copy code
rm -rf .terraform
Remove Lock File:

sh
Copy code
rm -f .terraform.lock.hcl
Reinitialize Terraform:

sh
Copy code
terraform init
After completing these steps, your Terraform working directory will be reset, and you can proceed with configuring your infrastructure from scratch.


checking usege 

(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId, State.Name]' --output table

---------------------------------------
|          DescribeInstances          |
+----------------------+--------------+
|  i-04f0419aa1b29772d |  terminated  |
+----------------------+--------------+
(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % aws ec2 describe-volumes --query 'Volumes[*].[VolumeId, State, Size, Attachments]' --output table

(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % aws ec2 describe-addresses --query 'Addresses[*].[PublicIp, InstanceId, AllocationId]' --output table

(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % aws elbv2 describe-load-balancers --output table

-----------------------
|DescribeLoadBalancers|
+---------------------+
(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % aws autoscaling describe-auto-scaling-groups --output table

---------------------------
|DescribeAutoScalingGroups|
+-------------------------+
(base) sagi@sagis-MacBook-Pro ecs_auto_deployment- % 

   __|  __|  __|
   _|  (   \__ \   Amazon Linux 2 (ECS Optimized)
 ____|\___|____/

For documentation, visit http://aws.amazon.com/documentation/ecs





 docker inspect e61ac1046640 | grep -i oom

checking cluster name passed:
cat /etc/ecs/ecs.config | grep ECS_CLUSTER

 to fix:
 http.response.body=
  | <?xml version="1.0" encoding="UTF-8"?>
  | <Response><Errors><Error><Code>DependencyViolation</Code><Message>Network vpc-0d4526c92c584d2fd has some mapped public address(es). Please unmap those public address(es) before detaching the gateway.</Message></Error></Errors><RequestID>1afd93a6-c7be-4ee9-97ec-ee963b5f6c25</RequestID></Response>



           

#sudo reboot

# ECS_ENABLE_SPOT_INSTANCE_DRAINING=true
# ECS_CONTAINER_STOP_TIMEOUT=120s
# ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=1h
# ECS_IMAGE_CLEANUP_INTERVAL=10m
# ECS_RESERVED_MEMORY=32 # need to ckeck-----------------------------------
# ECS_DISABLE_IMAGE_CLEANUP=false
# ECS_ENABLE_UNTRACKED_IMAGE_CLEANUP=true
# ECS_ENABLE_MEMORY_UNBOUNDED_WINDOWS_WORKAROUND=true
# ECS_DATADIR=/data
# ECS_ENABLE_GPU_SUPPORT=false
# ECS_POLL_METRICS=true
# ECS_UPDATE_DOWNLOAD_MAX_RETRIES=5
# ECS_ENABLE_AWSLOGS_EXECUTIONROLE_OVERRIDE=true

# #!/bin/bash
# dnf install nano -y
# # Redirect all output to a log file and console
# exec > >(tee /var/log/user-data.log | logger -t user-data -s 2>/dev/console) 2>&1

# echo "Starting enhanced user-data script at $(date)"

# # Function to check last command status and log
# check_status() {
#     local status=$?
#     if [ $status -ne 0 ]; then
#         echo "ERROR: $1 failed"
#         return 1
#     else
#         echo "SUCCESS: $1 completed successfully"
#         return 0
#     fi
# }

# # Function to check if a service is running
# check_service() {
#     if systemctl is-active --quiet "$1"; then
#         echo "SUCCESS: $1 is running"
#         return 0
#     else
#         echo "ERROR: $1 is not running"
#         systemctl status "$1"
#         return 1
#     fi
# }

# # Function to check if a file exists
# check_file() {
#     if [ -f "$1" ]; then
#         echo "SUCCESS: File $1 exists"
#         return 0
#     else
#         echo "ERROR: File $1 does not exist"
#         return 1
#     fi
# }

# # Function to check if a string is in a file
# check_string_in_file() {
#     if grep -q "$1" "$2"; then
#         echo "SUCCESS: String '$1' found in $2"
#         return 0
#     else
#         echo "ERROR: String '$1' not found in $2"
#         return 1
#     fi
# }

# # Function to check IAM role permissions
# check_iam_permission() {
#     if aws "$1" "$2" "$3" "$4" 2>&1 | grep -q "An error occurred"; then
#         echo "ERROR: IAM role does not have permission to perform $1 $2"
#         return 1
#     else
#         echo "SUCCESS: IAM role has permission to perform $1 $2"
#         return 0
#     fi
# }

# # Ensure the script is running on an ECS-optimized AMI
# if ! grep -q "Amazon Linux 2" /etc/os-release; then
#     echo "ERROR: This script is intended to run on an ECS-optimized Amazon Linux 2 AMI"
#     exit 1
# fi

# # Install necessary tools (if not already present)
# echo "Ensuring necessary tools are installed..."
# yum install -y jq aws-cli
# check_status "Tool installation"

# # Check IAM role permissions
# echo "Checking IAM role permissions..."
# region=$(curl -s http://169.254.169.254/latest/meta-data/placement/region)
# check_iam_permission "ecs" "list-clusters" "--region" "$region"
# check_iam_permission "logs" "describe-log-groups" "--region" "$region"
# check_iam_permission "ec2" "describe-instances" "--region" "$region"

# # Set up ECS config
# echo "Configuring ECS..."
# mkdir -p /etc/ecs
# cat << EOF > /etc/ecs/ecs.config
# ECS_CLUSTER=${cluster_name}
# ECS_AVAILABLE_LOGGING_DRIVERS=["awslogs", "json-file"]
# ECS_ENABLE_CONTAINER_METADATA=true
# ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=1h
# ECS_IMAGE_PULL_BEHAVIOR=once
# ECS_ENABLE_SPOT_INSTANCE_DRAINING=true
# EOF
# check_status "ECS configuration"
# check_file "/etc/ecs/ecs.config"
# check_string_in_file "ECS_CLUSTER=${cluster_name}" "/etc/ecs/ecs.config"

# # Configure and start ECS agent
# echo "Ensuring ECS agent is configured and running..."
# systemctl enable --now ecs
# systemctl restart ecs
# check_service "ecs"

# # Wait for ECS agent to fully start
# sleep 30

# # Configure and start Docker (if not already running)
# echo "Ensuring Docker is configured and running..."
# if ! systemctl is-active --quiet docker; then
#     systemctl enable docker
#     systemctl start docker
# fi
# check_service "docker"

# # Set up Docker log rotation
# echo "Configuring Docker log rotation..."
# cat > /etc/docker/daemon.json <<EOF
# {
#   "log-driver": "json-file",
#   "log-opts": {
#     "max-size": "10m",
#     "max-file": "5"
#   }
# }
# EOF
# systemctl restart docker
# check_status "Docker log rotation configuration"
# check_file "/etc/docker/daemon.json"

# # Configure CloudWatch agent
# echo "Configuring CloudWatch agent..."
# cat > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json << EOF
# {
#   "agent": {
#     "run_as_user": "root"
#   },
#   "logs": {
#     "logs_collected": {
#       "files": {
#         "collect_list": [
#           {
#             "file_path": "/var/log/messages",
#             "log_group_name": "${log_group_name}",
#             "log_stream_name": "{instance_id}/system-logs"
#           },
#           {
#             "file_path": "/var/log/ecs/ecs-init.log",
#             "log_group_name": "${log_group_name}",
#             "log_stream_name": "{instance_id}/ecs-init"
#           },
#           {
#             "file_path": "/var/log/ecs/ecs-agent.log",
#             "log_group_name": "${log_group_name}",
#             "log_stream_name": "{instance_id}/ecs-agent"
#           },
#           {
#             "file_path": "/var/log/user-data.log",
#             "log_group_name": "${log_group_name}",
#             "log_stream_name": "{instance_id}/user-data"
#           }
#         ]
#       }
#     }
#   },
#   "metrics": {
#     "metrics_collected": {
#       "cpu": {
#         "measurement": [
#           "usage_active",
#           "usage_system",
#           "usage_user"
#         ],
#         "metrics_collection_interval": 60
#       },
#       "memory": {
#         "measurement": [
#           "used_percent",
#           "used",
#           "total"
#         ],
#         "metrics_collection_interval": 60
#       },
#       "swap": {
#         "measurement": [
#           "used_percent",
#           "used",
#           "free"
#         ],
#         "metrics_collection_interval": 60
#       },
#       "disk": {
#         "measurement": [
#           "used_percent",
#           "used",
#           "total"
#         ],
#         "metrics_collection_interval": 60,
#         "resources": [
#           "/"
#         ]
#       }
#     }
#   }
# }
# EOF
# check_status "CloudWatch agent configuration"
# check_file "/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json"

# # Start CloudWatch agent
# echo "Starting CloudWatch agent..."
# /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json
# systemctl enable amazon-cloudwatch-agent
# systemctl start amazon-cloudwatch-agent
# check_service "amazon-cloudwatch-agent"

# # Verify network connectivity
# echo "Verifying network connectivity..."
# if ping -c 3 amazon.com &> /dev/null; then
#     echo "SUCCESS: Network is reachable"
# else
#     echo "ERROR: Network is not reachable"
#     exit 1
# fi

# # Check ECS agent connectivity
# echo "Checking ECS agent connectivity..."
# if timeout 120s bash -c 'until curl -s http://localhost:51678/v1/metadata; do sleep 5; done' &> /dev/null; then
#     echo "SUCCESS: ECS agent is responsive"
# else
#     echo "ERROR: ECS agent is not responsive"
#     systemctl status ecs
#     journalctl -u ecs
#     exit 1
# fi

# # Verify Docker is operational
# echo "Verifying Docker functionality..."
# if docker run --rm hello-world &> /dev/null; then
#     echo "SUCCESS: Docker is operational"
# else
#     echo "ERROR: Docker is not functioning properly"
#     exit 1
# fi

# # Check if instance is registered with ECS cluster
# echo "Checking ECS cluster registration..."
# instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
# cluster_check=$(aws ecs list-container-instances --cluster "${cluster_name}" --filter "ec2InstanceId==$instance_id" --region "$region")
# if [[ $(echo "$cluster_check" | jq '.containerInstanceArns | length') -gt 0 ]]; then
#     echo "SUCCESS: Instance is registered with ECS cluster ${cluster_name}"
# else
#     echo "ERROR: Instance is not registered with ECS cluster ${cluster_name}"
#     exit 1
# fi

# # Final check for ECS agent
# if ! systemctl is-active --quiet ecs; then
#     echo "ERROR: ECS agent is not running at the end of the script"
#     systemctl status ecs
#     journalctl -u ecs
#     exit 1
# fi

# echo "All checks completed. User-data script finished at $(date)"

# # Final status check
# final_status=0
# for service in ecs docker amazon-cloudwatch-agent; do
#     if ! systemctl is-active --quiet "$service"; then
#         echo "ERROR: $service is not running"
#         final_status=1
#     fi
# done

# if [ $final_status -eq 0 ]; then
#     echo "SUCCESS: User-data script executed successfully"
# else
#     echo "ERROR: User-data script encountered issues"
#     exit 1
# fi









# # Configure ECS
# log "Configuring ECS agent"
# sudo mkdir -p /etc/ecs
# sudo tee /etc/ecs/ecs.config > /dev/null << EOF
# ECS_CLUSTER=${cluster_name}
# ECS_LOGLEVEL=info
# ECS_AVAILABLE_LOGGING_DRIVERS=["awslogs","json-file"]
# ECS_ENABLE_TASK_IAM_ROLE=true
# ECS_ENABLE_TASK_IAM_ROLE_NETWORK_HOST=true
# ECS_ENABLE_CONTAINER_METADATA=true
# ECS_ENABLE_EXECUTE_COMMAND=true
# EOF
# check_status "ECS agent configuration"

# #configure and start CloudWatch agent
# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard
# sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json

# # # Configure awslogs
# # log "Configuring awslogs"
# # sudo sed -i "s/region = us-east-1/region = ${region}/" /etc/awslogs/awscli.conf
# # sudo sed -i "s/log_group_name = /log_group_name = ${log_group_name}/" /etc/awslogs/awslogs.conf
# # sudo sed -i "s/log_stream_name = /log_stream_name = ${log_stream_name}/" /etc/awslogs/awslogs.conf
# # check_status "awslogs configuration"

# # # Start services
# # log "Starting awslogs and ECS services"
# # sudo systemctl enable awslogsd.service ecs
# # sudo systemctl start amazon-cloudwatch-agent
# # sudo systemctl enable amazon-cloudwatch-agent # start on boot time
# # check_status "awslogs and ECS services start"

# # # Pull latest ECS agent image
# # log "Pulling latest ECS agent image"
# # docker pull amazon/amazon-ecs-agent:latest
# # check_status "ECS agent image pull"

# # Make environment variables persistent
# sudo tee /etc/profile.d/ecs_environment.sh > /dev/null << EOF
# export cluster_name="${cluster_name}"
# export log_group_name="${log_group_name}"
# export log_stream_name="${log_stream_name}"
# export region="${region}"
# EOF

# log "ECS instance configuration completed"

#  # Verify network connectivity
# echo "Verifying network connectivity..."
# if ping -c 3 amazon.com &> /dev/null; then
#     echo "SUCCESS: Network is reachable"
# else
#     echo "ERROR: Network is not reachable"
#     exit 1
# fi
# # Docker Hub login
# if [ -z "${dockerhub_username}" ] || [ -z "${dockerhub_password}" ]; then
#     log "Warning: Docker Hub credentials not provided. Skipping Docker login."
# else
#     log "Logging into Docker Hub"
#     echo "${dockerhub_password}" | docker login --username "$dockerhub_username" --password "$dockerhub_password"
#     check_status "Docker Hub login"
# fi

# # Check ECS agent connectivity
# echo "Checking ECS agent connectivity..."
# if timeout 120s bash -c 'until curl -s http://localhost:51678/v1/metadata; do sleep 5; done' &> /dev/null; then
#     echo "SUCCESS: ECS agent is responsive"
# else
#     echo "ERROR: ECS agent is not responsive"
#     systemctl status ecs
#     journalctl -u ecs
#     exit 1
# fi

# # Verify Docker is operational
# echo "Verifying Docker functionality..."
# if docker run --rm hello-world &> /dev/null; then
#     echo "SUCCESS: Docker is operational"
# else
#     echo "ERROR: Docker is not functioning properly"
#     exit 1
# fi

# # Check if instance is registered with ECS cluster
# echo "Checking ECS cluster registration..."
# instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
# cluster_check=$(aws ecs list-container-instances --cluster "${cluster_name}" --filter "ec2InstanceId==$instance_id" --region "$region")
# if [[ $(echo "$cluster_check" | jq '.containerInstanceArns | length') -gt 0 ]]; then
#     echo "SUCCESS: Instance is registered with ECS cluster ${cluster_name}"
# else
#     echo "ERROR: Instance is not registered with ECS cluster ${cluster_name}"
#     exit 1
# fi

# # Final check for ECS agent
# if ! systemctl is-active --quiet ecs; then
#     echo "ERROR: ECS agent is not running at the end of the script"
#     systemctl status ecs
#     journalctl -u ecs
#     exit 1
# fi

# echo "All checks completed. User-data script finished at $(date)"

# # Final status check
# final_status=0
# for service in ecs docker amazon-cloudwatch-agent; do
#     if ! systemctl is-active --quiet "$service"; then
#         echo "ERROR: $service is not running"
#         final_status=1
#     fi
# done

# if [ $final_status -eq 0 ]; then
#     echo "SUCCESS: User-data script executed successfully"
# else
#     echo "ERROR: User-data script encountered issues"
#     exit 1
# fi

# # Set up EC2 terminate handler
# log "Setting up EC2 terminate handler"
# cat << EOF > /etc/ec2-terminate-handler
# #!/bin/bash
# # Notify ECS that the instance is terminating
# ecs-init stop
# sleep 30
# EOF

# chmod +x /etc/ec2-terminate-handler

# # Register the terminate handler
# echo '/etc/ec2-terminate-handler' | sudo tee -a /etc/rc.d/rc.local
# sudo chmod +x /etc/rc.d/rc.local
# check_status "EC2 terminate handler setup"

# log "ECS instance configuration completed successfully"
# log "Rebooting instance in 10 seconds to apply all changes"
# sleep 10

# {
#   echo "ECS agent status:"
#   sudo docker ps | grep ecs-agent
#   sudo systemctl status ecs
# } >> /var/log/user-data.log

# # ecs checks!
# # Create a directory for the output
# mkdir -p /var/log/ecs_debug_output
# cd /var/log/ecs_debug_output || exit 1

# # Function to run command and save output
# run_command() {
#     echo "Running: $1"
#     eval "$1" > "$2" 2>&1
#     echo "Output saved to: $2"
#     echo
# }

# # Display current environment variables
# {
#     echo "Current environment variables:"
#     echo "cluster_name: $cluster_name"
#     echo "log_group_name: $log_group_name"
#     echo "log_stream_name: $log_stream_name"
#     echo "region: $region"
#     echo "dockerhub_username: $dockerhub_username"
#     echo "dockerhub_password: [REDACTED]"
# } >> 00_environment_vars.txt

# # Run checks and save outputs
# run_command "systemctl status ecs" "01_ecs_status.txt"
# run_command "tail -n 100 /var/log/ecs/ecs-agent.log" "02_ecs_agent_log.txt"
# run_command "curl -s http://localhost:51678/v1/metadata | python -mjson.tool" "03_instance_metadata.json"
# run_command "cat /var/lib/ecs/data/instance-attributes.json" "04_instance_attributes.json"
# run_command "cat /etc/ecs/ecs.config" "05_ecs_config.txt"
# run_command "docker info" "06_docker_info.txt"
# run_command "free -m" "07_memory_info.txt"
# run_command "df -h" "08_disk_info.txt"
# run_command "curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/" "09_iam_role.txt"
# run_command "aws ecs list-clusters --region $region" "10_ecs_clusters.json"
# run_command "aws ecs describe-task-definition --task-definition $cluster_name --region $region" "11_task_definition.json"
# run_command "aws ec2 describe-instance-attribute --instance-id $(curl -s http://169.254.169.254/latest/meta-data/instance-id) --attribute groupSet --region $region" "12_security_groups.json"
# run_command "aws ecs list-tasks --cluster $cluster_name --region $region" "13_ecs_tasks.json"
# run_command "systemctl status docker" "14_docker_status.txt"
# run_command "docker ps | grep ecs-agent" "15_ecs_agent_version.txt"
# run_command "ping -c 4 ecs.$region.amazonaws.com" "16_ecs_connectivity.txt"
# run_command "curl -v https://ecs.$region.amazonaws.com" "17_ecs_api_connectivity.txt"
# run_command "tail -n 100 /var/log/amazon/ecs/ecs-init.log" "18_ecs_init_log.txt"
# run_command "aws cloudwatch get-metric-statistics --metric-name CPUCreditBalance --namespace AWS/EC2 --period 300 --statistics Average --dimensions Name=InstanceId,Value=$(curl -s http://169.254.169.254/latest/meta-data/instance-id) --start-time $(date -u +"%Y-%m-%dT%H:%M:%SZ" --date "-1 hour") --end-time $(date -u +"%Y-%m-%dT%H:%M:%SZ") --region $region" "19_cpu_credits.json"
# run_command "journalctl -xe" "20_system_events.txt"
# run_command "aws ecs describe-container-instances --cluster $cluster_name --container-instances $(curl -s http://localhost:51678/v1/metadata | jq -r .ContainerInstanceArn) --region $region" "21_container_instance_state.json"
# run_command "aws ecs describe-services --services $cluster_name --cluster $cluster_name --region $region" "22_ecs_service_description.json"
# run_command "aws logs get-log-events --log-group-name $log_group_name --log-stream-name $log_stream_name --region $region" "23_cloudwatch_logs.json"
# run_command "docker login --username $dockerhub_username --password $dockerhub_password" "24_docker_login_test.txt"

# echo "All checks completed. Please review the output files in the '/var/log/ecs_debug_output' directory."

# # q
# # Docker Hub login
# if [ -z "${dockerhub_username}" ] || [ -z "${dockerhub_password}" ]; then
#     log "Warning: Docker Hub credentials not provided. Skipping Docker login."
# else
#     log "Logging into Docker Hub"
#     echo "${dockerhub_password}" |docker login --username "$dockerhub_username" --password "$dockerhub_password"
#     check_status "Docker Hub login"
# fi

# # Additional diagnostic checks
# log "Running diagnostic checks"

# # Check Docker status
# log "Checking Docker status"
# sudo systemctl status docker
# check_status "Docker status check"

# # Check ECS service status
# log "Checking ECS service status"
# sudo systemctl status ecs
# check_status "ECS service status check"

# # Check ECS agent logs
# log "Checking ECS agent logs"
# sudo tail -n 50 /var/log/ecs/ecs-agent.log
# check_status "ECS agent log check"

# # Verify ECS agent is running
# log "Verifying ECS agent is running"
# docker ps | grep ecs-agent
# check_status "ECS agent running check"

# # Check ECS agent metadata
# log "Checking ECS agent metadata"
# curl -s http://localhost:51678/v1/metadata
# check_status "ECS agent metadata check"

# # Check system resources
# log "Checking system resources"
# free -m
# df -h
# check_status "System resources check"

# # Check ECS cluster status
# log "Checking ECS cluster status"
# aws ecs describe-clusters --clusters "${cluster_name}" --region "${region}"
# check_status "ECS cluster status check"

# # List container instances
# log "Listing container instances"
# aws ecs list-container-instances --cluster "${cluster_name}" --region "${region}"
# check_status "Container instances list"

# # List tasks
# log "Listing tasks"
# aws ecs list-tasks --cluster "${cluster_name}" --region "${region}"
# check_status "Tasks list"

# # Check network connectivity
# log "Checking network connectivity"
# ping -c 4 "ecs.${region}.amazonaws.com"
# check_status "Network connectivity check"

# # View ECS config file
# log "Viewing ECS config file"
# cat /etc/ecs/ecs.config
# check_status "ECS config file check"

# log "Diagnostic checks completed"

# # Final status check
# final_status=0
# for service in ecs docker awslogsd; do
#     if ! systemctl is-active --quiet "$service"; then
#         log "ERROR: $service is not running"
#         final_status=1
#     fi
# done

# if [ $final_status -eq 0 ]; then
#     log "SUCCESS: User-data script executed successfully"
# else
#     log "ERROR: User-data script encountered issues"
#     exit 1
# fi


# # Debug output
# {
#     echo "Current environment variables:"
#     echo "cluster_name: $cluster_name"
#     echo "log_group_name: $log_group_name"
#     echo "log_stream_name: $log_stream_name"
#     echo "region: $region"
#     echo "dockerhub_username: $dockerhub_username"
#     echo "dockerhub_password: [REDACTED]"
# } >> /var/log/ecs_debug_output.txt


# log "ECS instance configuration and diagnostics completed"

# # Make environment variables persistent
# {
#     echo "export cluster_name=\"${cluster_name}\""
#     echo "export log_group_name=\"${log_group_name}\""
#     echo "export log_stream_name=\"${log_stream_name}\""
#     echo "export region=\"${region}\""
#     echo "export dockerhub_username=\"${dockerhub_username}\""
#     echo "export dockerhub_password=\"${dockerhub_password}\""
# } >> /etc/profile.d/ecs_environment.sh











#!/bin/bash 

# Set and export variables
export cluster_name="${cluster_name}"
export log_group_name="${log_group_name}"
export log_stream_name="${log_stream_name}"
export region="${region}"


# Write environment variables to /etc/environment
echo "cluster_name=${cluster_name}" | sudo tee -a /etc/environment
echo "log_group_name=${log_group_name}" | sudo tee -a /etc/environment
echo "log_stream_name=${log_stream_name}" | sudo tee -a /etc/environment
echo "region=${region}" | sudo tee -a /etc/environment



# Function to log messages
log() {
    echo "[$(date +'%Y-%m-%d %H:%M:%S')] $1" | tee -a /var/log/user-data.log
}

# Function to check status and log
check_status() {
    if [ $? -eq 0 ]; then
        log "Success: $1"
    else
        log "Error: $1 failed"
        exit 1
    fi
}

# Start configuration
log "Starting ECS instance configuration"
log "ECS Cluster Name: $cluster_name"
log "Log Group Name: $log_group_name"
log "Region: $region"

# Update system packages
log "Updating system packages"
sudo yum update -y
check_status "System update"



# Create CloudWatch agent configuration file
log "Creating CloudWatch agent configuration"
cat <<EOF > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json
{
    "agent": {
        "metrics_collection_interval": 60,
        "run_as_user": "root"
    },
    "logs": {
        "logs_collected": {
            "files": {
                "collect_list": [
                    {
                        "file_path": "/var/log/messages",
                        "log_group_name": "your-log-group-name",
                        "log_stream_name": "{instance_id}-messages"
                    },
                    {
                        "file_path": "/var/log/ecs/ecs-agent.log",
                        "log_group_name": "your-log-group-name",
                        "log_stream_name": "{instance_id}-ecs-agent"
                    },
                    {
                        "file_path": "/var/log/ecs/ecs-init.log",
                        "log_group_name": "your-log-group-name",
                        "log_stream_name": "{instance_id}-ecs-init"
                    }
                ]
            }
        }
    },
    "metrics": {
        "metrics_collected": {
            "cpu": {
                "measurement": [
                    "usage_active",
                    "usage_system",
                    "usage_user"
                ],
                "metrics_collection_interval": 60
            },
            "disk": {
                "measurement": [
                    "used_percent"
                ],
                "metrics_collection_interval": 60,
                "resources": [
                    "*"
                ]
            },
            "mem": {
                "measurement": [
                    "used_percent"
                ],
                "metrics_collection_interval": 60
            }
        }
    }
}
EOF
check_status "CloudWatch agent configuration creation"

# Start CloudWatch agent
log "Starting CloudWatch agent"
sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -s -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json
check_status "CloudWatch agent start"

#restart
sudo systemctl restart amazon-cloudwatch-agent

# Enable CloudWatch agent to start on boot
log "Enabling CloudWatch agent to start on boot"
sudo systemctl enable amazon-cloudwatch-agent
check_status "CloudWatch agent enable on boot"

# Configure ECS agent
log "Configuring ECS agent"
echo "ECS_CLUSTER=${cluster_name}" | sudo tee -a /etc/ecs/ecs.config
echo "ECS_AVAILABLE_LOGGING_DRIVERS=[\"awslogs\",\"json-file\"]" | sudo tee -a /etc/ecs/ecs.config
check_status "ECS agent configuration"

# Restart ECS agent to apply changes
log "Restarting ECS agent"
sudo systemctl restart ecs
check_status "ECS agent restart"

log "ECS instance configuration completed"



ami-06813d3793d203a47



# # Function to check service status with timeout
# check_service_status() {
#     local service_name="$1"
#     local max_attempts="$2"
#     local attempt=1

#     while [ "${attempt}" -le "${max_attempts}" ]; do
#         log "Checking ${service_name} status (attempt ${attempt}/${max_attempts})"
#         if systemctl is-active "${service_name}" >/dev/null 2>&1; then
#             log "${service_name} is running"
#             return 0
#         fi
#         attempt=$((attempt + 1))
#         sleep 5
#     done
    
#     error_log "${service_name} failed to start after ${max_attempts} attempts"
#     systemctl status "${service_name}" | tee -a "${ERROR_LOG}"
#     return 1
# }

# export cluster_name=cluster_name
# export log_group_name=log_group_name
# export log_stream_name=log_stream_name
# export region=us-east-1


# # Function to verify ECS agent connection
# verify_ecs_agent() {
#     log "Verifying ECS agent connection..."
    
#     local max_attempts=12
#     local attempt=1
#     local cluster_info
#     local detected_cluster
    
#     while [ "${attempt}" -le "${max_attempts}" ]; do
#         if curl -s localhost:51678/v1/metadata > /dev/null; then
#             cluster_info=$(curl -s localhost:51678/v1/metadata)
#             detected_cluster=$(echo "${cluster_info}" | jq -r '.Cluster' 2>/dev/null)
            
#             if [ "${detected_cluster}" = "${cluster_name}" ]; then
#                 log "ECS agent successfully connected to cluster ${cluster_name}"
#                 return 0
#             else
#                 error_log "Cluster name mismatch. Expected: ${cluster_name}, Got: ${detected_cluster}"
#             fi
#         fi
        
#         if [ "${attempt}" -eq "${max_attempts}" ]; then
#             error_log "Failed to verify ECS agent connection after ${max_attempts} attempts"
#             return 1
#         fi
        
#         log "Waiting for ECS agent connection (attempt ${attempt}/${max_attempts})..."
#         attempt=$((attempt + 1))
#         sleep 10
#     done
# }